"""
Real GenAI-Powered Vulnerability Scanner
Uses LLM (Groq/Llama) to discover cybersecurity gaps in water treatment systems
"""
import os
import json
import numpy as np
from typing import List, Dict, Optional
import requests


class LLMVulnerabilityScanner:
    """
    Uses LLM reasoning to discover vulnerabilities in industrial control systems
    """
    
    def __init__(self, api_key: Optional[str] = None, model: str = "llama-3.1-8b-instant"):
        """
        Initialize with Groq API (free tier)
        Get API key from: https://console.groq.com
        """
        self.api_key = api_key or os.getenv("GROQ_API_KEY")
        if not self.api_key:
            print("‚ö†Ô∏è  No GROQ_API_KEY found. Set it with: export GROQ_API_KEY='your_key'")
            print("   Get free key from: https://console.groq.com")
        
        self.model = model
        self.base_url = "https://api.groq.com/openai/v1/chat/completions"
        self.conversation_history = []
        
    def _call_llm(self, prompt: str, system_prompt: str = None, temperature: float = 0.7) -> str:

        messages = []

        if system_prompt:
            messages.append({
                "role": "system",
                "content": str(system_prompt)
            })

        messages.append({
            "role": "user",
            "content": str(prompt)
        })

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": "llama-3.1-8b-instant",
            "messages": messages,
            "temperature": float(temperature),
            "max_tokens": 1024,
            "stream": False
        }

        try:

            response = requests.post(
                "https://api.groq.com/openai/v1/chat/completions",
                headers=headers,
                json=payload
            )

            print("DEBUG STATUS:", response.status_code)
            print("DEBUG RESPONSE:", response.text)

            if response.status_code != 200:
                return None

            result = response.json()

            return result["choices"][0]["message"]["content"]

        except Exception as e:
            print(f"‚ùå LLM API Error: {e}")
            return None


    def analyze_system_architecture(self, sensor_data: Dict, process_description: str) -> Dict:
        """
        Use LLM to understand the system architecture and identify attack surfaces
        """
        system_prompt = """You are an expert cybersecurity analyst specializing in industrial control systems (ICS) and SCADA security. 
Your expertise includes:
- Water treatment plant operations and safety
- Cyber-physical system vulnerabilities
- Attack vectors in critical infrastructure
- Physics-based attack detection evasion

Analyze systems critically and identify realistic attack scenarios."""

        prompt = f"""Analyze this water treatment system for cybersecurity vulnerabilities:

SYSTEM DESCRIPTION:
{process_description}

AVAILABLE SENSORS:
{json.dumps(sensor_data, indent=2)}

TASK:
Identify 5 specific cybersecurity vulnerabilities or attack vectors that could:
1. Evade standard anomaly detection
2. Cause physical process failures
3. Be difficult to attribute
4. Exploit physics/chemistry of water treatment

For each vulnerability, provide:
- Attack vector name
- Target sensors/actuators
- Attack mechanism (how it works)
- Expected impact on water quality/safety
- Evasion strategy (how it avoids detection)
- Severity (LOW/MEDIUM/HIGH/CRITICAL)

Return as JSON array with this structure:
[
  {{
    "name": "vulnerability name",
    "target": "sensor/actuator",
    "mechanism": "detailed attack mechanism",
    "impact": "physical impact on process",
    "evasion": "how it evades detection",
    "severity": "CRITICAL/HIGH/MEDIUM/LOW"
  }}
]

Return ONLY valid JSON, no markdown."""

        print("ü§ñ Analyzing system with AI...")
        response = self._call_llm(prompt, system_prompt, temperature=0.5)
        
        if response:
            try:
                # Extract JSON from response (in case LLM adds markdown)
                json_start = response.find('[')
                json_end = response.rfind(']') + 1
                json_str = response[json_start:json_end]
                vulnerabilities = json.loads(json_str)
                return {
                    "vulnerabilities": vulnerabilities,
                    "raw_analysis": response
                }
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è  Could not parse JSON: {e}")
                print(f"Response: {response[:500]}")
                return {"vulnerabilities": [], "raw_analysis": response}
        
        return {"vulnerabilities": [], "raw_analysis": None}
    
    def generate_attack_vector(self, vulnerability: Dict, sensor_ranges: Dict) -> Dict:
        """
        Use LLM to generate specific attack parameters for a vulnerability
        """
        system_prompt = """You are a penetration tester specializing in cyber-physical systems.
Generate realistic attack parameters that could exploit identified vulnerabilities.
Be specific with sensor values and timing."""

        prompt = f"""Generate specific attack parameters for this vulnerability:

VULNERABILITY:
{json.dumps(vulnerability, indent=2)}

SENSOR RANGES (normalized 0-1):
{json.dumps(sensor_ranges, indent=2)}

TASK:
Design a concrete attack that exploits this vulnerability.

Provide:
1. Target sensor values (as array of 14 normalized values 0-1)
2. Attack timing/duration
3. Expected detection difficulty (1-10, 10=hardest to detect)
4. Step-by-step attack execution
5. Why this specific configuration evades detection

Return as JSON:
{{
  "sensor_values": [14 float values between 0-1],
  "duration_seconds": integer,
  "detection_difficulty": integer (1-10),
  "execution_steps": ["step1", "step2", ...],
  "evasion_reasoning": "why this works"
}}

Return ONLY valid JSON."""

        response = self._call_llm(prompt, system_prompt, temperature=0.6)

        if response:
            try:

                # Remove markdown
                response = response.replace("```json", "")
                response = response.replace("```", "")

                # Remove comments (// ...)
                lines = response.split("\n")
                clean_lines = []

                for line in lines:
                    if "//" in line:
                        line = line.split("//")[0]
                    clean_lines.append(line)

                clean_response = "\n".join(clean_lines)

                # Extract JSON
                json_start = clean_response.find('{')
                json_end = clean_response.rfind('}') + 1
                json_str = clean_response[json_start:json_end]

                attack_params = json.loads(json_str)

                return attack_params

            except Exception as e:
                print("JSON parse error:", e)
                print("Clean response:", clean_response)
                return None

        return None
    
    def explain_detection_failure(self, attack_vector: np.ndarray, 
                                  detected: bool, severity: str) -> str:
        """
        Use LLM to explain why an attack was/wasn't detected
        """
        system_prompt = """You are explaining cybersecurity detection results to operators.
Be concise and actionable."""

        status = "WAS DETECTED" if detected else "EVADED DETECTION"
        
        prompt = f"""Explain this detection result:

ATTACK VECTOR: {attack_vector.tolist()}
RESULT: {status}
ATTACK SEVERITY: {severity}

In 2-3 sentences, explain:
1. Why this attack {status.lower()}
2. What this means for system security
3. Recommended action

Be concise and clear."""

        response = self._call_llm(prompt, system_prompt, temperature=0.3)
        return response if response else "Analysis unavailable"
    
    def discover_gaps(self, detection_results: List[Dict]) -> Dict:
        """
        Analyze detection results to find systematic gaps
        """
        system_prompt = """You are analyzing cybersecurity detection system performance.
Identify patterns in failures and recommend improvements."""

        # Summarize results
        total = len(detection_results)
        detected = sum(1 for r in detection_results if r['detected'])
        missed = total - detected
        
        missed_attacks = [r for r in detection_results if not r['detected']]
        
        prompt = f"""Analyze these detection results to find security gaps:

SUMMARY:
- Total attacks tested: {total}
- Detected: {detected}
- Missed: {missed}
- Detection rate: {detected/total*100:.1f}%

MISSED ATTACKS:
{json.dumps(missed_attacks, indent=2)}

TASK:
Identify systematic gaps in the detection system.

Provide:
1. Common patterns in missed attacks
2. Root cause of detection failures
3. Specific improvements to make
4. Priority ranking (1=most urgent)

Return as JSON:
{{
  "gaps": [
    {{
      "pattern": "what's being missed",
      "root_cause": "why it's being missed",
      "improvement": "how to fix it",
      "priority": integer (1-5)
    }}
  ],
  "overall_assessment": "summary",
  "critical_actions": ["action1", "action2"]
}}

Return ONLY valid JSON."""

        response = self._call_llm(prompt, system_prompt, temperature=0.4)
        
        if response:
            try:
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                json_str = response[json_start:json_end]
                return json.loads(json_str)
            except:
                return {"gaps": [], "overall_assessment": response}
        
        return {"gaps": [], "overall_assessment": None}


# Example usage and testing
if __name__ == "__main__":
    # Test with dummy data
    scanner = LLMVulnerabilityScanner()
    
    sensor_data = {
        "FIT101": "Raw water flow sensor",
        "LIT101": "Raw water tank level",
        "FIT201": "Chemical dosing flow",
        "AIT201": "Water conductivity sensor",
        "FIT301": "Filtered water flow",
        "LIT301": "Filtered water tank level"
    }
    
    process_desc = """
    Secure Water Treatment (SWaT) testbed with 6 stages:
    P1: Raw water intake and storage
    P2: Chemical dosing
    P3: Ultrafiltration
    P4: Dechlorination (RO feed)
    P5: Reverse osmosis
    P6: Backwash and cleaning
    
    Critical safety constraints:
    - Tank levels must stay 0.2-0.9 to prevent overflow/underflow
    - Flow rates must balance to prevent pressure spikes
    - Conductivity must stay in safe range for water quality
    """
    
    print("Testing LLM Vulnerability Scanner...")
    print("=" * 60)
    
    if scanner.api_key:
        result = scanner.analyze_system_architecture(sensor_data, process_desc)
        print("\n‚úì Found vulnerabilities:")
        for vuln in result.get('vulnerabilities', []):
            print(f"\n  üéØ {vuln.get('name', 'Unknown')}")
            print(f"     Severity: {vuln.get('severity', 'UNKNOWN')}")
            print(f"     Target: {vuln.get('target', 'N/A')}")
    else:
        print("\n‚ö†Ô∏è  Set GROQ_API_KEY environment variable to test")
        print("   Get free key: https://console.groq.com")
